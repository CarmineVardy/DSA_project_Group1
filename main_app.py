'''
Script: main_app.py
Digital Health Systems and Applications - Project work 2025-2026
Description:
Main Streamlit application acting as the Clinical Decision Support System (CDSS) interface.
It orchestrates the entire workflow:
1. Connects to the FHIR Server and retrieves patient data.
2. Uses the resource wrapper classes to build a comprehensive clinical context.
3. Initializes and interacts with the LLM (Bio-Medical-Llama-3) for clinical reasoning.
4. Manages the chat interface, history persistence, and CDA document generation/upload.

Group: Carmine Vardaro, Marco Savastano, Francesco Ferrara.
'''

import os
import json
import base64
import sys
import re
import html
import streamlit as st
import torch
import transformers
import huggingface_hub
from fhirpy import SyncFHIRClient
from dotenv import load_dotenv
from datetime import datetime

# Resource wrappers
from resources.administration.patient import AppPatient
from resources.administration.device import AppDevice
from resources.clinical.allergyIntolerance import AppAllergyIntolerance
from resources.clinical.carePlan import AppCarePlan
from resources.clinical.condition import AppCondition
from resources.clinical.procedure import AppProcedure
from resources.diagnostics.diagnosticReport import AppDiagnosticReport
from resources.diagnostics.documentReference import AppDocumentReference
from resources.diagnostics.observation import AppObservation
from resources.medications.immunization import AppImmunization
from resources.medications.medication import AppMedication
from resources.medications.medicationRequests import AppMedicationRequest

# Configuration loading
load_dotenv()
SERVER_URL = os.getenv("SERVER_URL")
HF_TOKEN = os.getenv("HF_TOKEN")
HISTORY_FILE = "chat_history.json"

def format_patient_dropdown_label(p: AppPatient) -> str:
    """
    Formats the patient label for the selection dropdown.
    Format: [Icon] Name | Gender | DOB
    """
    icon = "üî¥" if p.is_deceased else "üü¢"     
    gender_map = {'male': 'M', 'female': 'F', 'other': 'O', 'unknown': '?'}
    
    # Handle gender extraction (enum or string)
    gender_val = p.gender.value if hasattr(p.gender, 'value') else p.gender
    g = gender_map.get(gender_val, '?')
    
    dob = p.birth_date.strftime('%Y-%m-%d') if p.birth_date else "Unknown"
    
    # Clean up name (remove ordinal prefixes if present, e.g., "1st: John")
    raw_name = p.full_name.split('\n')[0]
    clean_name = raw_name.split(':', 1)[1].strip() if ":" in raw_name else raw_name
    
    return f"{icon} {clean_name} | {g} | {dob}"

def clean_medical_markdown(text: str) -> str:
    """
    Cleans and standardizes Markdown text generated by the LLM or extracted from notes.
    Fixes headers and bullet point formatting.
    """
    if not text: return ""
    # Normalize headers
    text = re.sub(r'^\s*#+\s+', '##### ', text, flags=re.MULTILINE)
    # Fix bullet points following bold text
    text = re.sub(r'(\*\*.+?\)\s*)(\- )', r'\1\n\2', text)
    return text

def load_json_history() -> dict:
    """
    Loads the chat history from the local JSON file.
    """
    if not os.path.exists(HISTORY_FILE): 
        return {}
    try:
        with open(HISTORY_FILE, "r") as f: 
            return json.load(f)
    except Exception: 
        return {}

def save_json_history(history_dict: dict):
    """
    Saves the chat history to the local JSON file.
    """
    with open(HISTORY_FILE, "w") as f: 
        json.dump(history_dict, f, indent=4)

@st.cache_resource(show_spinner="Initializing Dr. Llama...")
def load_llm():
    """
    Initializes the HuggingFace text-generation pipeline.
    Uses the Bio-Medical-Llama-3-8B model with bfloat16 precision.
    """
    if not HF_TOKEN: 
        return None
    
    huggingface_hub.login(HF_TOKEN)
    
    return transformers.pipeline(
        "text-generation", 
        model="ContactDoctor/Bio-Medical-Llama-3-8B", 
        model_kwargs={"dtype": torch.bfloat16, "low_cpu_mem_usage": True}, 
        device_map="auto"
    )

@st.cache_data(show_spinner="Downloading data from FHIR Server...")
def get_patient_clinical_context(patient_json: dict, _client):
    """
    Orchestrates the retrieval of all clinical resources for a selected patient.
    1. Parses the base Patient resource.
    2. Fetches Medications first to build a resolution map.
    3. Iterates through all other resource types (Conditions, Observations, etc.),
       fetches them, and adds them to the patient object.
    4. Generates the final text summary (prompt) for the LLM.
    """
    try:
        selected_patient = AppPatient(patient_json)
    except Exception as e:
        return f"Error parsing patient data: {e}", {}, "N/A"
        
    counts = {}
    medication_map = {} 

    # --- Step 1: Fetch Medications for resolution ---
    try:
        med_bundle = _client.resources('MedicationRequest') \
                            .search(patient=selected_patient.id) \
                            .include('MedicationRequest', 'medication') \
                            .fetch_raw()
                            
        if med_bundle and med_bundle.entry:
            for entry in med_bundle.entry:
                res = entry.resource
                if res.resource_type == 'Medication':
                    try:
                        app_med = AppMedication(res.serialize())
                        medication_map[res.id] = app_med
                    except Exception as e:
                        print(f"[WARNING] Could not parse Medication {res.id}: {e}")
    except Exception as e:
        st.error(f"[ERROR] Error fetching medications map: {e}")
    
    # --- Step 2: Fetch all other clinical resources ---
    resource_configs = [
        ('Device',            AppDevice,            selected_patient.add_devices),
        ('AllergyIntolerance',AppAllergyIntolerance,selected_patient.add_allergies),
        ('CarePlan',          AppCarePlan,          selected_patient.add_care_plans),
        ('Condition',         AppCondition,         selected_patient.add_conditions),
        ('Procedure',         AppProcedure,         selected_patient.add_procedures),
        ('DiagnosticReport',  AppDiagnosticReport,  selected_patient.add_diagnostic_reports),
        ('DocumentReference', AppDocumentReference, selected_patient.add_document_references),
        ('Observation',       AppObservation,       selected_patient.add_observations),
        ('Immunization',      AppImmunization,      selected_patient.add_immunizations),
        ('MedicationRequest', AppMedicationRequest, selected_patient.add_medication_requests)
    ]

    for resource_type, AppClass, add_method in resource_configs:
        fetched_resources = []
        try:
            # Fetch resources sorted by last updated to get recent data first
            fetched_resources = _client.resources(resource_type) \
                                       .search(patient=selected_patient.id) \
                                       .sort('-_lastUpdated') \
                                       .fetch_all()
            counts[resource_type] = len(fetched_resources)
        except Exception as e:
            print(f"[ERROR] Error fetching {resource_type} for patient {selected_patient.id}: {e}")
            counts[resource_type] = 0
            continue
            
        app_objects = []
        for res in fetched_resources:
            try:
                app_obj = AppClass(res.serialize())
                app_objects.append(app_obj)
            except Exception as e:
                r_id = res.id if hasattr(res, 'id') else "Unknown"
                print(f"[WARNING] Could not parse {resource_type} {r_id}: {e}")

        add_method(app_objects)

    return selected_patient.generate_clinical_context(medication_map), counts, selected_patient.age

def generate_cda_xml(app_patient, title, content):
    """
    Generates a valid HL7 CDA (Clinical Document Architecture) XML document 
    containing the LLM's response.
    """
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    
    # Birth Date Handling
    if app_patient.birth_date:
        try:
            dob = app_patient.birth_date.strftime("%Y%m%d")
        except:
            dob = str(app_patient.birth_date).replace("-", "")[0:8]
    else:
        dob = "00000000"

    p_id = app_patient.id
    try:
        name_entry = app_patient.resource.name[0]
        given = " ".join(name_entry.given or [])
        family = name_entry.family or ""
    except:
        given = "Unknown"
        family = "Patient"
        
    gender_code = "M" if app_patient.gender == 'male' else "F" if app_patient.gender == 'female' else "UN"

    # Escape HTML characters in content to prevent XML breakage
    safe_content = html.escape(content)
    
    paragraphs_xml = ""
    blocks = safe_content.split("\n")
    for block in blocks:
        block = block.strip()
        if block:
            paragraphs_xml += f"<paragraph>{block}</paragraph>\n"

    cda_template = f"""<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="CDA.xsl"?>
<ClinicalDocument xmlns="urn:hl7-org:v3" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
    <typeId root="2.16.840.1.113883.1.3" extension="POCD_HD000040"/>
    <templateId root="2.16.840.1.113883.2.4.6.10.100001"/>
    <id root="2.16.840.1.113883.2.4.6.3" extension="{timestamp}"/>
    <code code="11488-4" codeSystem="2.16.840.1.113883.6.1" displayName="Consultation Note"/>
    <title>{title}</title>
    <effectiveTime value="{timestamp}"/>
    <confidentialityCode code="N" codeSystem="2.16.840.1.113883.5.25"/>
    <languageCode code="en-US"/>
    
    <recordTarget>
        <patientRole>
            <id root="2.16.840.1.113883.2.4.6.3" extension="{p_id}"/>
            <patient>
                <name>
                    <given>{given}</given>
                    <family>{family}</family>
                </name>
                <administrativeGenderCode code="{gender_code}" codeSystem="2.16.840.1.113883.5.1"/>
                <birthTime value="{dob}"/>
            </patient>
        </patientRole>
    </recordTarget>

    <author>
        <time value="{timestamp}"/>
        <assignedAuthor>
            <id root="2.16.840.1.113883.2.4.6.3" extension="LLM_AI_V1"/>
            <assignedPerson>
                <name>
                    <given>Dr.</given>
                    <family>Llama AI</family>
                </name>
            </assignedPerson>
        </assignedAuthor>
    </author>

    <custodian>
        <assignedCustodian>
            <representedCustodianOrganization>
                <id root="2.16.840.1.113883.2.4.6.1" extension="FHIR_HOSPITAL"/>
                <name>FHIR Smart Hospital</name>
            </representedCustodianOrganization>
        </assignedCustodian>
    </custodian>

    <component>
        <structuredBody>
            <component>
                <section>
                    <title>AI Clinical Response</title>
                    <text>
                        {paragraphs_xml}
                    </text>
                </section>
            </component>
        </structuredBody>
    </component>
</ClinicalDocument>"""
    return cda_template

def upload_cda_to_fhir(cda_xml, title, patient_id, client):
    """
    Uploads the generated CDA document to the FHIR Server as a DocumentReference resource.
    """
    try:
        encoded_content = base64.b64encode(cda_xml.encode('utf-8')).decode('utf-8')
        
        doc_ref = {
            'resourceType': 'DocumentReference',
            'status': 'current',
            'docStatus': 'final',
            'type': {'text': 'AI Consultation Note'},
            'subject': {'reference': f'Patient/{patient_id}'},
            'date': datetime.now().isoformat(),
            'description': title,
            'content': [{
                'attachment': {
                    'contentType': 'text/xml',
                    'data': encoded_content,
                    'title': f"{title}.xml"
                }
            }]
        }
        client.resource('DocumentReference', **doc_ref).save()
        return True
    except Exception as e:
        print(f"Error uploading to FHIR: {e}")
        return False

# --- STREAMLIT UI CONFIGURATION ---
st.set_page_config(page_title="FHIR Chatbot - BioLlama", layout="wide")

# Custom CSS for a cleaner look
st.markdown("""
    <style>
        div.block-container {
            padding-top: 1rem !important; /* Was much larger by default */
            padding-bottom: 1rem !important;
        }
        header[data-testid="stHeader"] {
            height: 2rem !important; /* Reduce system header height */
        }
        div[data-testid="stSidebarHeader"] {
            padding-top: 0px !important;
            padding-bottom: 0px !important;
            height: 3rem !important;
        }
        div[data-testid="stSidebarUserContent"] {
            padding-top: 0rem !important;
        }
        div[data-baseweb="select"] * {
            cursor: pointer !important;
        }
        .small-badge {
            font-size: 0.85rem;
            padding: 2px 8px;
            border-radius: 12px;
            display: inline-flex;
            align-items: center;
            font-weight: 500;
        }
        .badge-green { background-color: #d1fae5; color: #065f46; border: 1px solid #34d399; }
        .badge-red { background-color: #fee2e2; color: #991b1b; border: 1px solid #f87171; }
        .dot { height: 8px; width: 8px; border-radius: 50%; display: inline-block; margin-right: 6px; }
        .dot-green { background-color: #059669; }
        .dot-red { background-color: #dc2626; }
        p { margin-bottom: 0.2rem; }
        hr.compact { margin: 0.5rem 0 !important; border-top: 1px solid #e6e6e6; }
        div[data-testid="column"] { padding: 0 !important; }
        div[data-testid="stAlert"] p { margin-bottom: 1rem !important; }
    </style>
""", unsafe_allow_html=True)

st.title("üí¨ Dr. Llama - CDSS Assistant")

# --- INITIALIZATION ---
if "history_per_patient" not in st.session_state:
    st.session_state.history_per_patient = load_json_history()

llm = load_llm()

try:
    client = SyncFHIRClient(SERVER_URL)
except Exception:
    st.error("Could not connect to FHIR Server. Check .env")
    st.stop()

# --- SIDEBAR: PATIENT SELECTION & CONTEXT ---
with st.sidebar:
    st.title("üè• Patient Selection")
    try:
        # Fetch recent patients for dropdown
        raw_patients = client.resources('Patient').sort('_lastUpdated').limit(100).fetch()   
        patient_options = {}
        for raw_p in raw_patients:
            try:
                temp_p = AppPatient(raw_p.serialize())
                label = format_patient_dropdown_label(temp_p)
                # Ensure uniqueness in dropdown keys
                unique_key = f"{label} ##{temp_p.id}" 
                patient_options[unique_key] = raw_p.serialize()
            except Exception: pass

        if not patient_options:
            st.warning("No patients found.")
        else:
            selected_key = st.selectbox(
                label="Search patient", 
                options=list(patient_options.keys()),
                format_func=lambda x: x.split(" ##")[0],
                index=None, 
                placeholder="üîç Search patient...", 
                label_visibility="collapsed"
            )    
            
            if selected_key:
                patient_data_json = patient_options[selected_key]
                app_patient = AppPatient(patient_data_json)
                pid = app_patient.id                
                
                # --- Patient Demographics Display ---
                st.markdown('<hr class="compact">', unsafe_allow_html=True)
                st.caption(f"FHIR ID: `{pid}`")
                st.markdown(f"### {app_patient.full_name}")
                
                c1, c2, c3 = st.columns([1.2, 0.8, 1])
                with c1:
                    dob = app_patient.birth_date.strftime('%d/%m/%Y') if app_patient.birth_date else "N/A"
                    st.write(f"**Born:** {dob}")
                with c2:
                    g = app_patient.gender.value.capitalize() if app_patient.gender else "?"
                    st.write(f"**Sex:** {g}")
                with c3:
                    # AGE PLACEHOLDER (Waiting for calculation via clinical context)
                    age_placeholder = st.empty()
                    age_placeholder.write("**Age:** ...") 

                st.write("") 
                if app_patient.is_deceased:
                    st.markdown('<div class="small-badge badge-red"><span class="dot dot-red"></span>DECEASED</div>', unsafe_allow_html=True)
                else:
                    st.markdown('<div class="small-badge badge-green"><span class="dot dot-green"></span>ACTIVE PATIENT</div>', unsafe_allow_html=True)

                st.markdown('<div style="height: 15px;"></div>', unsafe_allow_html=True)
                
                # --- Fetch Clinical Context ---
                calculated_age = "N/A"
                clinical_context_str, fetched_counts, calculated_age = get_patient_clinical_context(patient_data_json, client)
                
                # Update Age Placeholder
                if calculated_age != "N/A" and calculated_age != -1:
                    age_placeholder.write(f"**Age:** {calculated_age}")
                else:
                    age_placeholder.write("**Age:** ?")

                # --- Clinical Records Stats ---
                st.markdown('<hr class="compact">', unsafe_allow_html=True)
                st.markdown("**üìÇ Clinical Records**")
                
                required_res = [ "Device", "AllergyIntolerance", "CarePlan", "Condition", "Procedure", "DiagnosticReport", "DocumentReference", "Observation", "Immunization" ]
                
                col_left, col_right = st.columns(2)
                left_content = ""
                right_content = ""
                
                for idx, r_type in enumerate(required_res):
                    count = fetched_counts.get(r_type, 0)
                    display_name = r_type.replace("MedicationRequest", "Medications").replace("AllergyIntolerance", "Allergies").replace("DocumentReference", "Documents")
                    line = f"<div style='line-height:1.2; font-size:0.9em;'>{display_name}: <b>{count}</b></div>"
                    
                    if idx % 2 == 0: left_content += line
                    else: right_content += line
                
                with col_left: st.markdown(left_content, unsafe_allow_html=True)
                with col_right: st.markdown(right_content, unsafe_allow_html=True)

                # --- LLM Context Preview ---
                st.markdown('<hr class="compact">', unsafe_allow_html=True)
                st.markdown("### üß† LLM Context View")
                
                cleaned_context = clean_medical_markdown(clinical_context_str)
                with st.container(height=500):
                    st.markdown(cleaned_context)

                # --- Reset Button ---
                st.markdown('<hr class="compact">', unsafe_allow_html=True)
                if st.button("üóëÔ∏è Reset Local Chat", use_container_width=True):
                    if pid in st.session_state.history_per_patient:
                        st.session_state.history_per_patient[pid] = []
                        save_json_history(st.session_state.history_per_patient)
                    st.rerun()

    except Exception as e:
        st.error(f"Connection/Loading Error: {e}")

# --- CHAT LOGIC ---
if 'pid' in locals() and llm and clinical_context_str:
    # Initialize history for the specific patient if not present
    if pid not in st.session_state.history_per_patient:
        st.session_state.history_per_patient[pid] = []
    
    history = st.session_state.history_per_patient[pid]

    # --- 1. DISPLAY HISTORY ---
    for i, msg in enumerate(history):
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            
            # UI for Assistant: CDA Management
            if msg["role"] == "assistant":
                st.markdown("<div style='margin-bottom: 0.5rem;'></div>", unsafe_allow_html=True)
                
                # Unique name based on content/timestamp
                # We use a base title. If strict timestamp is needed, retrieve from JSON.
                auto_title = f"Consultation_Note_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}"

                # PERSISTENCE LOGIC:
                # Check if this message was already saved (flag in JSON)
                is_already_saved = msg.get("is_saved", False)
                saved_xml_content = msg.get("xml_content", "") # Retrieve XML if exists

                # STATE 1: NOT YET SAVED (Neither in session nor JSON)
                if not is_already_saved:
                    if st.button("‚òÅÔ∏è Save to FHIR & Prepare Download", key=f"btn_save_{i}", use_container_width=True):
                        # 1. Generate XML
                        xml_content = generate_cda_xml(app_patient, auto_title, msg["content"])
                        
                        # 2. Upload to FHIR
                        with st.spinner("Uploading to FHIR Server..."):
                            if upload_cda_to_fhir(xml_content, auto_title, pid, client):
                                
                                # --- FIX CACHE ---
                                # Tell Streamlit to invalidate cache for patient data
                                # Next recalculation will fetch the new DocumentReference
                                get_patient_clinical_context.clear()
                                
                                # --- FIX PERSISTENCE ---
                                # Mark message as saved in history
                                history[i]["is_saved"] = True
                                history[i]["xml_content"] = xml_content # Save XML for download after refresh
                                history[i]["saved_title"] = auto_title
                                
                                # Write immediately to disk
                                st.session_state.history_per_patient[pid] = history
                                save_json_history(st.session_state.history_per_patient)
                                
                                st.success("Uploaded & Cached!")
                                st.rerun() # Reload page (updating sidebar counters!)
                            else:
                                st.error("Failed to upload to FHIR. Check connection.")

                # STATE 2: ALREADY SAVED (Retrieve data from JSON)
                else:
                    col_msg, col_dwn = st.columns([1, 4])
                    with col_msg:
                        st.success("‚úÖ Saved!")
                    with col_dwn:
                        # Use XML saved in JSON for consistency
                        # If old message without saved xml, regenerate on the fly
                        xml_to_download = saved_xml_content if saved_xml_content else generate_cda_xml(app_patient, "Recovered_Note", msg["content"])
                        title_to_download = msg.get("saved_title", auto_title)

                        st.download_button(
                            label="‚¨áÔ∏è Download XML File (CDA)",
                            data=xml_to_download,
                            file_name=f"{title_to_download}.xml",
                            mime="text/xml",
                            key=f"btn_dwn_{i}",
                            use_container_width=True
                        )
                
                st.markdown("---")

    # --- 2. PROMPT PREPARATION ---
    system_message = f"""You are Bio-Medical-Llama-3, an expert AI assistant specialized in Clinical Decision Support.
Your goal is to analyze the provided patient data and answer clinical questions with high accuracy and safety.

CRITICAL GUIDELINES:
1. **Reference Date & Timeline:** The Patient Clinical Summary below starts with a line formatted as `(Current Date: YYYY-MM-DD)`. You must EXTRACT this date and assume it is the **current present day**. All patient ages, event timelines, and "active" statuses must be interpreted relative to this specific date. Do not use the real-world current date.
2. **Evidence-Based:** Base your recommendations on standard clinical guidelines (e.g., ACC/AHA, ESC, ADA) relevant to the patient's condition.
3. **Strict Context:** Answer ONLY based on the provided patient summary. Do not assume information not present in the text.
4. **Safety & Risk Analysis (MANDATORY):** - You MUST rigorously check for both **drug-drug interactions** AND **drug-disease contraindications** (e.g., renal impairment, liver disease, hypertension).
   - When identifying a risk, you MUST **explain the specific mechanism** (e.g., "increases risk of bleeding") and **cite the exact data** from the summary (e.g., "due to concurrent use of Warfarin and history of Gastric Ulcer").
5. **Therapeutic Alternatives:** If a requested drug is contraindicated or unsafe given the patient's context, you MUST proactively suggest safer **alternative medications** or strategies when possible.
6. **Tone:** Professional, objective, and concise.

--- BEGIN PATIENT CLINICAL SUMMARY ---
{clinical_context_str}
--- END PATIENT CLINICAL SUMMARY ---
"""

    # --- 3. MEMORY CALCULATION (Preventive) ---
    # Reconstruct history formatted FOR THE MODEL (not for UI)
    model_history = [{"role": "system", "content": system_message}]
    for msg in history:
        content = msg["content"]
        # If user, add prefix ONLY HERE
        if msg["role"] == "user":
            content = f"CLINICAL QUESTION:\n{content}"
        model_history.append({"role": msg["role"], "content": content})

    temp_prompt_ids = llm.tokenizer.apply_chat_template(model_history, tokenize=True, add_generation_prompt=True)
    current_tokens = len(temp_prompt_ids)
    model_limit = 8192
    safety_margin = 512 
    remaining_space = model_limit - current_tokens

    # Update Sidebar Memory Usage
    with st.sidebar:
        st.markdown('<hr class="compact">', unsafe_allow_html=True)
        st.write("üìä **Memory Usage**")
        st.progress(min(1.0, current_tokens / model_limit), text=f"{current_tokens} / {model_limit} tokens")
        
    # --- 4. INPUT HANDLING ---
    if remaining_space < safety_margin:
        st.warning("‚ö†Ô∏è **Conversation limit reached.** The model context is full. Please use the 'Reset Local Chat' button in the sidebar to start a new session.")
    else:
        if prompt := st.chat_input("Enter clinical question..."):
            
            # A. SHOW AND SAVE CLEAN MESSAGE (For User UI)
            history.append({"role": "user", "content": prompt})
            with st.chat_message("user"): 
                st.markdown(prompt)

            # B. PREPARE FORMATTED MESSAGE (For AI)
            formatted_user_msg = f"CLINICAL QUESTION:\n{prompt}"
            
            # C. GENERATION
            with st.chat_message("assistant"):
                with st.spinner("Analyzing patient data..."):
                    
                    # Create final messages for inference
                    # Note: reconstruct history for model including the last formatted message
                    final_messages_for_llm = [{"role": "system", "content": system_message}]
                    
                    # Add old formatted messages
                    for old_msg in history[:-1]: # All except the last one just added
                        c = old_msg["content"]
                        if old_msg["role"] == "user":
                            c = f"CLINICAL QUESTION:\n{c}"
                        final_messages_for_llm.append({"role": old_msg["role"], "content": c})
                    
                    # Add the last message (current) formatted
                    final_messages_for_llm.append({"role": "user", "content": formatted_user_msg})

                    prompt_ids = llm.tokenizer.apply_chat_template(final_messages_for_llm, tokenize=False, add_generation_prompt=True)
                    
                    terminators = [
                        llm.tokenizer.eos_token_id,
                        llm.tokenizer.convert_tokens_to_ids("<|eot_id|>")
                    ]

                    outputs = llm(
                        prompt_ids, 
                        max_new_tokens=min(1024, remaining_space),
                        eos_token_id=terminators, 
                        do_sample=True, 
                        temperature=0.6, 
                        top_p=0.9,
                    )

                    response = outputs[0]["generated_text"][len(prompt_ids):].strip()
                    
                    st.markdown(response)
                    
                    # D. SAVE RESPONSE
                    history.append({"role": "assistant", "content": response})
                    st.session_state.history_per_patient[pid] = history
                    save_json_history(st.session_state.history_per_patient)
                    st.rerun()

else:
    # --- NO PATIENT SELECTED ---
    st.markdown("<div style='height: 0vh;'></div>", unsafe_allow_html=True)
    st.info("üëà Please select a patient from the sidebar to start.")